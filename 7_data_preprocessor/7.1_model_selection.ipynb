{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "colours = sns.color_palette(\"Set2\")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from model_tester import test_embedding_models\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from utils import get_table_from_supabase\n",
    "\n",
    "#get keys from env\n",
    "load_dotenv()\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipient Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until this point, the database was limited to recipient names and activities. I have chosen to extend this to include recipient objectives to hopefully enrich the data and provide another context to test for the embedding model selection. I have updated the database builders, so that objectives can be imported from the start in future iterations, and also created `2_recipients_table_builder/recipient_objectives_importer.py` so that I do not have to re-build the entire database at this point in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Data from Supabase and Building Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will connect to Supabase and retrieve the relevant records, in order to isolate the text data that I will use to embed and select the best model. For this purpose, I will use the 12 funder-recipient pairs that I have curated for my evaluation app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get evaluation pairs and extract ids\n",
    "evaluation_pairs = get_table_from_supabase(url, key, \"evaluation_pairs\")\n",
    "funder_ids = evaluation_pairs[\"funder_registered_num\"].unique()\n",
    "eval_recip_ids = evaluation_pairs[\"recipient_id\"].unique()\n",
    "\n",
    "#add my ratings to compare later\n",
    "ratings = {\n",
    "    1: 0.25,\n",
    "    2: 0.60,\n",
    "    3: 0.25,\n",
    "    4: 0.25,\n",
    "    5: 0.10,\n",
    "    6: 0.60,\n",
    "    7: 0.60,\n",
    "    8: 0.25,\n",
    "    9: 0.25,\n",
    "    10: 0.25,\n",
    "    11: 0.80,\n",
    "    12: 0.30\n",
    "}\n",
    "evaluation_pairs[\"my_rating\"] = evaluation_pairs[\"id\"].map(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the 12 funders and recipients\n",
    "funders = get_table_from_supabase(url, key, \"funders\")\n",
    "funders = funders[funders[\"registered_num\"].isin(funder_ids)]\n",
    "\n",
    "recipients = get_table_from_supabase(url, key, \"recipients\")\n",
    "recipients = recipients[recipients[\"recipient_id\"].isin(eval_recip_ids)]\n",
    "\n",
    "#create dataframes\n",
    "recipients_df = recipients.copy()\n",
    "funders_df = funders.copy()\n",
    "\n",
    "#add embedding columns to funder/recipient dfs\n",
    "funders_df[\"embeddings\"] = None\n",
    "recipients_df[\"embeddings\"] = None\n",
    "\n",
    "#add funder and recipient names to pairs df\n",
    "evaluation_pairs = evaluation_pairs.merge(\n",
    "    funders_df[[\"registered_num\", \"name\"]],\n",
    "    left_on=\"funder_registered_num\",\n",
    "    right_on=\"registered_num\",\n",
    "    how=\"left\"\n",
    ")\n",
    "evaluation_pairs = evaluation_pairs.rename(columns={\"name\": \"funder_name\"})\n",
    "evaluation_pairs = evaluation_pairs.drop(columns=[\"registered_num\"])\n",
    "evaluation_pairs = evaluation_pairs.merge(\n",
    "    recipients_df[[\"recipient_id\", \"recipient_name\"]],\n",
    "    on=\"recipient_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dfs\n",
    "print(f\"Recipients: {recipients_df.shape} | Funders: {funders_df.shape} | Evaluation Pairs: {evaluation_pairs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint - save dfs to pickle\n",
    "# recipients_df.to_pickle(\"recipients_df.pkl\")\n",
    "# funders_df.to_pickle(\"funders_df.pkl\")\n",
    "# evaluation_pairs.to_pickle(\"evaluation_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipients_df = pd.read_pickle(\"recipients_df.pkl\")\n",
    "funders_df = pd.read_pickle(\"funders_df.pkl\")\n",
    "evaluation_pairs = pd.read_pickle(\"evaluation_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model Evaluation by Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to test four models on my small evaluation dataset: \n",
    "- `all-MiniLM-L6-v2`\n",
    "- `snowflake-arctic-embed-s`\n",
    "- `all-roberta-large-v1`\n",
    "- `bge-large-en-v1.5` \n",
    "\n",
    "Snowflake's model is generally finetuned for retrieval but I have decided to include it as it has performed so highly against other benchmarks and compared to major competitors (Merrick et al, 2024). The other models are consistent with the analysis by Pavlyshenko and Stasiuk (2025), who found these architectures to be reliable on semantic similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add model names\n",
    "models = [\"all-MiniLM-L6-v2\", \"Snowflake/snowflake-arctic-embed-s\", \"all-roberta-large-v1\", \"BAAI/bge-large-en-v1.5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Activities Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare text columns\n",
    "recipients_df[\"recipients_text\"] = recipients_df[\"recipient_activities\"].fillna(\"\").str.lower()\n",
    "funders_df[\"funders_text\"] = funders_df[\"activities\"].fillna(\"\").str.lower()\n",
    "\n",
    "results_act = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "results_act_df, pairs_act = test_embedding_models(\n",
    "    models_list=models,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    evaluation_pairs=evaluation_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_act_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view pairs with scores from each model\n",
    "pairs_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Objectives Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare text columns\n",
    "recipients_df[\"recipients_text\"] = recipients_df[\"recipient_objectives\"].fillna(\"\").str.lower()\n",
    "funders_df[\"funders_text\"] = funders_df[\"objectives\"].fillna(\"\").str.lower()\n",
    "\n",
    "results_obj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "results_obj_df, pairs_obj = test_embedding_models(\n",
    "    models_list=models,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    evaluation_pairs=evaluation_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_obj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Activities and Objectives (API Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare text columns\n",
    "recipients_df[\"recipients_text\"] = recipients_df[\"recipient_activities\"].fillna(\"\") + \"\" + recipients_df[\"recipient_objectives\"].fillna(\"\").str.lower()\n",
    "funders_df[\"funders_text\"] = funders_df[\"activities\"].fillna(\"\") + \"\" + funders_df[\"objectives\"].fillna(\"\").str.lower()\n",
    "\n",
    "results_ao_api = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "results_ao_api, pairs_ao_api = test_embedding_models(\n",
    "    models_list=models,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    evaluation_pairs=evaluation_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ao_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_ao_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 - Activities and Objectives (Extracted and API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare text columns\n",
    "recipients_df[\"recipients_text\"] = recipients_df[\"recipient_activities\"].fillna(\"\") + \"\" + recipients_df[\"recipient_objectives\"].fillna(\"\").str.lower()\n",
    "funders_df[\"funders_text\"] = funders_df[\"activities\"].fillna(\"\") + \"\" + funders_df[\"objectives\"].fillna(\"\") + \"\" + funders_df[\"objectives_activities\"].fillna(\"\").str.lower()\n",
    "\n",
    "results_ao_ext = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "results_ao_ext, pairs_ao_ext = test_embedding_models(\n",
    "    models_list=models,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    evaluation_pairs=evaluation_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ao_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_ao_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make scatterplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_col, title) in enumerate([\n",
    "    (\"all-MiniLM-L6-v2_sim\", \"MiniLM\"),\n",
    "    (\"Snowflake/snowflake-arctic-embed-s_sim\", \"Snowflake Arctic Embed\"),\n",
    "    (\"all-roberta-large-v1_sim\", \"Roberta\"),\n",
    "    (\"BAAI/bge-large-en-v1.5_sim\", \"BGE-M3\")\n",
    "]):\n",
    "    axes[idx].scatter(evaluation_pairs[\"my_rating\"], evaluation_pairs[model_col],\n",
    "                        alpha=0.6, s=100, color=colours[idx])\n",
    "\n",
    "    #show line of best fit\n",
    "    z = np.polyfit(evaluation_pairs[\"my_rating\"], evaluation_pairs[model_col], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(evaluation_pairs[\"my_rating\"].sort_values(),\n",
    "                    p(evaluation_pairs[\"my_rating\"].sort_values()),\n",
    "                    \"r-\", alpha=0.5, linewidth=2)\n",
    "\n",
    "    axes[idx].set_xlabel(\"My Rating\")\n",
    "    axes[idx].set_ylabel(\"Model Similarity Score\")\n",
    "    axes[idx].set_title(f\"{title}\\nCorrelation: {results_df.iloc[idx]['correlation']:.3f}\")\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

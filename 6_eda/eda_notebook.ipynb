{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from eda_utils import add_gbp_columns, explode_lists, get_longest_values, print_in_rows\n",
    "from stats_builder import make_summary_df, calculate_stats, make_calculated_df, format_stats, format_df\n",
    "from plots_builder import make_bar_chart\n",
    "from utils import get_table_from_supabase\n",
    "\n",
    "#get keys from env\n",
    "load_dotenv()\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Data from Supabase and Building Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will connect to Supabase and retrieve all records, in order to start building my analysis dataframes. I will create one dataframe for funder information, and another for grants and recipients information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tables and build dataframes\n",
    "tables = [\"funders\", \"causes\", \"areas\", \"beneficiaries\", \"grants\",\n",
    "               \"funder_causes\", \"funder_areas\", \"funder_beneficiaries\", \"funder_grants\", \n",
    "               \"financials\", \"funder_financials\"]\n",
    "\n",
    "for table in tables:\n",
    "    globals()[table] = get_table_from_supabase(url, key, table)\n",
    "\n",
    "#get recipients with filter\n",
    "recipients = get_table_from_supabase(url, key, \"recipients\", batch_size=50, filter_recipients=True)\n",
    "all_recipient_ids = set(recipients[\"recipient_id\"].unique())\n",
    "\n",
    "#get and filter recipient join tables\n",
    "recipient_join_tables = [\"recipient_grants\", \"recipient_areas\", \"recipient_beneficiaries\", \"recipient_causes\"]\n",
    "for table in recipient_join_tables:\n",
    "    df = get_table_from_supabase(url, key, table)\n",
    "    globals()[table] = df[df[\"recipient_id\"].isin(all_recipient_ids)]\n",
    "\n",
    "print(f\"Loaded {len(funders)} funders, {len(recipients)} recipients, {len(grants)} grants\")\n",
    "print(f\"Filtered recipient join tables to {len(all_recipient_ids)} valid recipients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Funders Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funders_df = funders.copy()\n",
    "\n",
    "#define table relationships for funders\n",
    "funder_rels = [\n",
    "    {\n",
    "        \"join_table\": funder_causes,\n",
    "        \"lookup_table\": causes,\n",
    "        \"key\": \"cause_id\",\n",
    "        \"value_col\": \"cause_name\",\n",
    "        \"result_col\": \"causes\"\n",
    "    },\n",
    "    {\n",
    "        \"join_table\": funder_areas,\n",
    "        \"lookup_table\": areas,\n",
    "        \"key\": \"area_id\",\n",
    "        \"value_col\": \"area_name\",\n",
    "        \"result_col\": \"areas\"\n",
    "    },\n",
    "    {\n",
    "        \"join_table\": funder_beneficiaries,\n",
    "        \"lookup_table\": beneficiaries,\n",
    "        \"key\": \"ben_id\",\n",
    "        \"value_col\": \"ben_name\",\n",
    "        \"result_col\": \"beneficiaries\"\n",
    "    }\n",
    "]\n",
    "\n",
    "#group and merge\n",
    "for rel in funder_rels:\n",
    "    grouped = rel[\"join_table\"].merge(rel[\"lookup_table\"], on=rel[\"key\"])\n",
    "    grouped = grouped.groupby(\"registered_num\")[rel[\"value_col\"]].apply(list).reset_index()\n",
    "    grouped.columns = [\"registered_num\", rel[\"result_col\"]]\n",
    "    funders_df = funders_df.merge(grouped, on=\"registered_num\", how=\"left\")\n",
    "\n",
    "#add grant statistics columns \n",
    "grants_stats = funder_grants.merge(grants, on=\"grant_id\")\n",
    "grants_agg = grants_stats.groupby(\"registered_num\").agg({\n",
    "    \"grant_id\": \"count\",\n",
    "    \"amount\": [\"sum\", \"mean\", \"median\"]\n",
    "}).reset_index()\n",
    "grants_agg.columns = [\"registered_num\", \"num_grants\", \"total_given\", \"avg_grant\", \"median_grant\"]\n",
    "\n",
    "funders_df = funders_df.merge(grants_agg, on=\"registered_num\", how=\"left\")\n",
    "funders_df[\"num_grants\"] = funders_df[\"num_grants\"].astype(\"Int64\")\n",
    "\n",
    "#replace nan values with empty lists\n",
    "funders_df[\"causes\"] = funders_df[\"causes\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "funders_df[\"areas\"] = funders_df[\"areas\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "funders_df[\"beneficiaries\"] = funders_df[\"beneficiaries\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "#round to 2 decimal places\n",
    "funders_df = funders_df.round(2)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "#format financial columns\n",
    "float_cols = [\"income_latest\", \"expenditure_latest\", \"total_given\", \"avg_grant\", \"median_grant\"]\n",
    "for col in float_cols:\n",
    "    if col in funders_df.columns:\n",
    "        funders_df[col + \"_gbp\"] = funders_df[col].apply(add_gbp_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial History Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get full financial records and separate into income and expenditure\n",
    "financial_history = funder_financials.merge(financials, on=\"financials_id\")\n",
    "income_history = financial_history[financial_history[\"financials_type\"] == \"income\"]\n",
    "expenditure_history = financial_history[financial_history[\"financials_type\"] == \"expenditure\"]\n",
    "\n",
    "#make financials dicts\n",
    "income_by_funder = income_history.groupby(\"registered_num\").apply(\n",
    "    lambda x: dict(zip(x[\"financials_year\"], x[\"financials_value\"]))\n",
    ").reset_index()\n",
    "income_by_funder.columns = [\"registered_num\", \"income_history\"]\n",
    "\n",
    "expenditure_by_funder = expenditure_history.groupby(\"registered_num\").apply(\n",
    "    lambda x: dict(zip(x[\"financials_year\"], x[\"financials_value\"]))\n",
    ").reset_index()\n",
    "expenditure_by_funder.columns = [\"registered_num\", \"expenditure_history\"]\n",
    "\n",
    "#merge with funders and replace nans\n",
    "funders_df = funders_df.merge(income_by_funder, on=\"registered_num\", how=\"left\")\n",
    "funders_df = funders_df.merge(expenditure_by_funder, on=\"registered_num\", how=\"left\")\n",
    "funders_df[\"income_history\"] = funders_df[\"income_history\"].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "funders_df[\"expenditure_history\"] = funders_df[\"expenditure_history\"].apply(lambda x: x if isinstance(x, dict) else {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The List Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list entries\n",
    "list_entries = get_table_from_supabase(url, key, \"list_entries\")\n",
    "funder_list = get_table_from_supabase(url, key, \"funder_list\")\n",
    "list_with_info = funder_list.merge(list_entries, on=\"list_id\")\n",
    "\n",
    "#get list of entries for each funder\n",
    "list_grouped = list_with_info.groupby(\"registered_num\")[\"list_info\"].apply(list).reset_index()\n",
    "list_grouped.columns = [\"registered_num\", \"list_entries\"]\n",
    "\n",
    "#merge with funders and replace nans\n",
    "funders_df = funders_df.merge(list_grouped, on=\"registered_num\", how=\"left\")\n",
    "funders_df[\"list_entries\"] = funders_df[\"list_entries\"].apply(lambda x: x if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend column view, sort and preview funders\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "funders_df = funders_df.sort_values(\"total_given_gbp\", ascending=False)\n",
    "funders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Grants Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_df = grants.copy()\n",
    "\n",
    "#add funder info\n",
    "grants_df = grants_df.merge(funder_grants, on=\"grant_id\")\n",
    "grants_df = grants_df.merge(funders[[\"registered_num\", \"name\"]], on=\"registered_num\")\n",
    "grants_df = grants_df.rename(columns={\"name\": \"funder_name\"})\n",
    "grants_df = grants_df.rename(columns={\"registered_num\": \"funder_num\"})\n",
    "\n",
    "#add recipient info  \n",
    "grants_df = grants_df.merge(recipient_grants, on=\"grant_id\")\n",
    "grants_df = grants_df.merge(recipients[[\"recipient_id\", \"recipient_name\", \"recipient_activities\"]], \n",
    "                        on=\"recipient_id\", \n",
    "                        how=\"left\")\n",
    "\n",
    "#add recipient areas\n",
    "recip_areas_grouped = recipient_areas.merge(areas, on=\"area_id\")\n",
    "recip_areas_grouped = recip_areas_grouped.groupby(\"recipient_id\")[\"area_name\"].apply(list).reset_index()\n",
    "recip_areas_grouped.columns = [\"recipient_id\", \"recipient_areas\"]\n",
    "grants_df = grants_df.merge(recip_areas_grouped, on=\"recipient_id\", how=\"left\")\n",
    "\n",
    "#add recipient causes\n",
    "recip_causes_grouped = recipient_causes.merge(causes, on=\"cause_id\")\n",
    "recip_causes_grouped = recip_causes_grouped.groupby(\"recipient_id\")[\"cause_name\"].apply(list).reset_index()\n",
    "recip_causes_grouped.columns = [\"recipient_id\", \"recipient_causes\"]\n",
    "grants_df = grants_df.merge(recip_causes_grouped, on=\"recipient_id\", how=\"left\")\n",
    "\n",
    "#add recipient beneficiaries\n",
    "recip_beneficiaries_grouped = recipient_beneficiaries.merge(beneficiaries, on=\"ben_id\")\n",
    "recip_beneficiaries_grouped = recip_beneficiaries_grouped.groupby(\"recipient_id\")[\"ben_name\"].apply(list).reset_index()\n",
    "recip_beneficiaries_grouped.columns = [\"recipient_id\", \"recipient_beneficiaries\"]\n",
    "grants_df = grants_df.merge(recip_beneficiaries_grouped, on=\"recipient_id\", how=\"left\")\n",
    "\n",
    "#replace nan values with empty lists\n",
    "if \"recipient_areas\" in grants_df.columns:\n",
    "    grants_df[\"recipient_areas\"] = grants_df[\"recipient_areas\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "if \"recipient_causes\" in grants_df.columns:\n",
    "    grants_df[\"recipient_causes\"] = grants_df[\"recipient_causes\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "if \"recipient_beneficiaries\" in grants_df.columns:\n",
    "    grants_df[\"recipient_beneficiaries\"] = grants_df[\"recipient_beneficiaries\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "#add source of grant\n",
    "grants_df[\"source\"] = grants_df[\"grant_id\"].apply(lambda x: \"Accounts\" if str(x).startswith(\"2\") else \"360Giving\")\n",
    "\n",
    "#round to 2 decimal places\n",
    "grants_df = grants_df.round(2)\n",
    "\n",
    "#format financial columns\n",
    "grants_df[\"amount_gbp\"] = grants_df[\"amount\"].apply(add_gbp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort and preview grants\n",
    "grants_df = grants_df.sort_values(\"grant_title\", ascending=True)\n",
    "grants_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build df of summary statistics\n",
    "summary_data = make_summary_df(funders_df, grants_df)\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df[\"Value\"] = summary_df.apply(format_stats, axis=1)\n",
    "summary_df = format_df(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculated Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get calculated stats and build df\n",
    "stats = calculate_stats(funders_df, grants_df)\n",
    "calculated_data = make_calculated_df(stats)\n",
    "calculated_df = pd.DataFrame(calculated_data)\n",
    "calculated_df[\"Value\"] = calculated_df.apply(format_stats, axis=1)\n",
    "calculated_df = format_df(calculated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will view the structure of each dataframe to check for missing data and confirm that datatypes are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Missingness in Funders Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funders_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing activities - check manually\n",
    "missing_activities = funders_df[funders_df[\"activities\"].isna()][\"registered_num\"].tolist()\n",
    "print(missing_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing sections - check proportion of funders with sections extracted from accounts\n",
    "has_sections = funders_df[\n",
    "    funders_df[\"objectives_activities\"].notna() |\n",
    "    funders_df[\"achievements_performance\"].notna() |\n",
    "    funders_df[\"grant_policy\"].notna()\n",
    "]\n",
    "\n",
    "has_sections_total = len(has_sections)\n",
    "has_sections_proportion = has_sections_total / 996\n",
    "\n",
    "print(f\"Funders with accessible accounts: 327\")\n",
    "print(f\"Funders with extracted sections: {has_sections_total}\")\n",
    "print(f\"Proportion of funders with sections: {has_sections_proportion:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Findings from Missingness Analysis (Funders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five funders in the database with empty `activities`. I have checked the Charity Commission website and it does appear that these funders have simply not declared any activities. They are all relatively new having submitted only one set of accounts. The `activities_objectives` column has not been populated for any of these funders, which is unfortunate as this could have provided a further source of information. One of these funders has a website so, if I am able to achieve my stretch target of scraping websites, this may offer details (although at the time of writing, the website does not exist).\n",
    "\n",
    "It is expected that a significant proportion of funders would not have a website, and no action is required to address this.\n",
    "\n",
    "**Problem with Accounts**\n",
    "\n",
    "My database building scripts (for PDFs) have a serious limitation as I am unable to scrape the Charity Commission website for accounts where the page contains JavaScript. The older pages, which are basic HTML, are accessible but the newer ones are not, and I am unable to tell which charities have been updated to the new system until the script attempts to scrape them and fails. Having built the database, I can now see from the print statements that 327 funders have accessible accounts, representing 32.8% of the 996 funders in the sample. My calculations above show that the required sections have been extracted from accounts for 24.4% of funders - varying from 52 funders with `grant_policy`, to 194 funders with `achievements_peformance` and 230 funders with `objectives_activities`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Missingness in Grants Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing grant title and description - check numbers per source\n",
    "missing_by_source = grants_df.groupby(\"source\").agg({\n",
    "    \"grant_title\": lambda x: x.isna().sum(),\n",
    "    \"grant_desc\": lambda x: x.isna().sum(),\n",
    "    \"grant_id\": \"count\"\n",
    "}).rename(columns={\"grant_id\": \"total_grants\", \"grant_title\": \"missing_title\", \"grant_desc\": \"missing_desc\"})\n",
    "\n",
    "print(missing_by_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing grant amounts - spotcheck manually\n",
    "missing_amounts = grants_df[grants_df[\"amount\"].isna()][\"funder_num\"].tolist()\n",
    "print(missing_amounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing recipient activities - compare by source\n",
    "missing_recips = grants_df[\"recipient_activities\"].isna().sum()\n",
    "accounts_recips = grants_df[\n",
    "    (grants_df[\"source\"] == \"Accounts\") &\n",
    "    (grants_df[\"recipient_activities\"].isna())\n",
    "]\n",
    "missing_360g = grants_df[\n",
    "    (grants_df[\"source\"] == \"360Giving\") &\n",
    "    (grants_df[\"recipient_activities\"].isna())\n",
    "]\n",
    "\n",
    "print(f\"Grants extracted from accounts: {len(accounts_recips):,} ({len(accounts_recips)/missing_recips*100:.1f}%)\")\n",
    "print(f\"360Giving recipients with missing activities: {len(missing_360g):,} ({len(missing_360g)/missing_recips*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Findings from Missingness Analysis (Grants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 33,147 rows in the dataframe, 287 more than the number of unique grants as some grants are given to multiple recipients. The deduplication script in `stats_builder.py` ensures that the value of each grant has only been counted once for the purpose of the summary/calculated statistics.\n",
    "\n",
    "I have explored the missing data from `grant_title` and `grant_desc` to check that only grants extracted from accounts are missing these fields. This was to be expected as these details are not available for the grants from accounts; the figures above confirm that every grant from the 360Giving API is complete with a title and description. \n",
    "\n",
    "Of the 32,815 grants in the database, 46 (0.14%) are missing `amount`. I have performed a spot-check of 12 grants (25%), which suggests that these are genuine extraction errors from funder accounts rather than systematic issues. I have decided to keep these 46 grants in the database as they still provide valuable information about funder-recipient relationships and giving patterns, and such a small number of missing amounts is not likely to affect analyses/models.\n",
    "\n",
    "I also explored the 17,318 rows in `grants_df` with missing `recipient_activities`. Null values in this column are to be expected for recipients that are added following extraction from PDFs, which accounts for 11,486 (66.3%). The remaining 5,832 (33.7%) null values - which are present within records added to the database by the 360Giving API - can be explained by:\n",
    "- Charities that have been removed from the Charity Commission, e.g. if they have closed down\n",
    "- Registered charities that simply do not declare their activities (as present in 5 records in `funders_df`)\n",
    "- Where abnormalities are evident in `registered_num`. For example, the RSPB is identified in the 360Giving API by *207076 & SC037654* - its registered numbers for both the Charity Commission of England & Wales and the Office of the Scottish Charity Regulator. My database builder script was unable to match on *207076 & SC037654* as `registered_num`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check the lengths of the shortest and longest text entries, to ensure that they have been imported correctly and are not too short or long for unexpected reasons. Many funders provide very short explanations of their activities/objectives etc., such as simply \"grant-giving\" which is just one word - so this would not be abnormal. I will confirm that particularly long entries are not corrupted or the result of multiple documents being combined accidentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Word Counts in Funders Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check word counts for text columns in funders df\n",
    "funders_text_cols = [\"activities\", \"objectives\", \"objectives_activities\", \"achievements_performance\", \"grant_policy\"]\n",
    "\n",
    "#create columns\n",
    "for col in funders_text_cols:\n",
    "    funders_df[f\"word_count_{col}\"] = funders_df[col].str.split().str.len()\n",
    "for col in funders_text_cols:\n",
    "    print(f\"{col.upper()}\")\n",
    "    print(f\"{'_'*30}\\n\")\n",
    "\n",
    "    word_count_col = f\"word_count_{col}\"\n",
    "    not_nas = funders_df[funders_df[word_count_col].notna()]\n",
    "\n",
    "    #get minimums and maximums for each text column\n",
    "    if len(not_nas) > 0:\n",
    "        min_idx = not_nas[word_count_col].idxmin()\n",
    "        max_idx = not_nas[word_count_col].idxmax()\n",
    "\n",
    "        examples = funders_df.loc[[min_idx, max_idx], [\"registered_num\", \"name\", word_count_col, col]]\n",
    "        examples.index = [\"Minimum\", \"Maximum\"]\n",
    "\n",
    "        display(examples)\n",
    "    else:\n",
    "        print(\"No data available\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Counts for Sections Returned from Charity Commission API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high word counts - check top 10 longest sections from API (to manually check)\n",
    "long_activities = get_longest_values(funders_df, \"word_count_activities\", \"registered_num\")\n",
    "long_objectives = get_longest_values(funders_df, \"word_count_objectives\", \"registered_num\")\n",
    "\n",
    "api_sections_long = [\n",
    "    (\"activities\", long_activities),\n",
    "    (\"objectives\", long_objectives)\n",
    "]\n",
    "\n",
    "for name, longest in api_sections_long:\n",
    "    print(f\"Top 10 longest {name}: {longest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low word counts - count and view short sections from API\n",
    "api_sections_short = {\n",
    "    \"activities\": \"Short activities sections\",\n",
    "    \"objectives\": \"Short objectives sections\"\n",
    "}\n",
    "\n",
    "for col, title in api_sections_short.items():\n",
    "    funders_df[f\"short_{col}_section\"] = (\n",
    "        (funders_df[col].str.split().str.len() < 5) &\n",
    "        (funders_df[col].str.upper().str.strip() != \"GENERAL CHARITABLE PURPOSES\")\n",
    "    )\n",
    "    short_sections = funders_df[funders_df[f\"short_{col}_section\"]][col].dropna().unique()\n",
    "    \n",
    "    print(f\"\\n{title}: {funders_df[f'short_{col}_section'].sum():,}\")\n",
    "    print_in_rows(short_sections, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low word counts - check activities containing \"NONE\"\n",
    "activities_none = funders_df[\"activities\"].str.contains(\"NONE\", na=False)\n",
    "funders_df[activities_none]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Counts for Sections Extracted from PDF Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high word counts - check top 10 longest extracted sections (to manually check)\n",
    "long_obj_act = get_longest_values(funders_df, \"word_count_objectives_activities\", \"registered_num\")\n",
    "long_ach_perf = get_longest_values(funders_df, \"word_count_achievements_performance\", \"registered_num\")\n",
    "long_grant_policy = get_longest_values(funders_df, \"word_count_grant_policy\", \"registered_num\")\n",
    "\n",
    "accounts_sections_long = [\n",
    "    (\"objectives_activities\", long_obj_act),\n",
    "    (\"achievements_performance\", long_ach_perf),\n",
    "    (\"grant_policy\", long_grant_policy)\n",
    "]\n",
    "\n",
    "for name, longest in accounts_sections_long:\n",
    "    print(f\"Top 10 longest {name}: {longest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low word counts - count and view short sections from accounts\n",
    "accounts_sections_short = {\n",
    "    \"objectives_activities\": \"Short objectives_activities sections\",\n",
    "    \"achievements_performance\": \"Short achievements_performance sections\",\n",
    "    \"grant_policy\": \"Short grant_policy sections\"\n",
    "}\n",
    "\n",
    "for col, title in accounts_sections_short.items():\n",
    "    funders_df[f\"short_{col}_section\"] = (\n",
    "        (funders_df[col].str.split().str.len() < 20)\n",
    "    )\n",
    "    short_sections = funders_df[funders_df[f\"short_{col}_section\"]][col].dropna().unique()\n",
    "    \n",
    "    print(f\"\\n{title}: {funders_df[f'short_{col}_section'].sum():,}\")\n",
    "    print_in_rows(short_sections, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Findings from Word Count Analysis (Funders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sections from API**\n",
    "\n",
    "Having manually checked the maximum values against their Charity Commission records, I am confident that there are no entries that are concerningly long. The top 10 longest `activities` are not abnormal; they are just relatively wordy compared to others. I viewed the shortest values (fewer than five words) and concluded that they are acceptable, although I will reassign the values containing \"NONE\" to N/A to improve the quality of the embedding.\n",
    "\n",
    "**Sections from Accounts**\n",
    "\n",
    "The word count analysis has highlighted that there are numerous issues with the sections that have been extracted from accounts. Many that are too long contain multiple sections, whereas many that are too short have been cut off prematurely. There is also a problem of repetition - the extractor sometimes extracts the same (excessively long) chunk of text for both `objectives_activities` and `achievements_performance`, defeating the purpose. Looking at the top 10 shortest and longest has demonstrated that the regex extraction technique employed in the database builder has not worked very well. Rather than rebuilding the entire PDF extraction pipeline, I will address this data quality issue by re-processing the extracted text through an LLM-based extractor with the aim of better handling the variation in section sizes and formatting. The LLM should be better at the fuzzy matching that regex has struggled with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Word Counts in Grants Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check word counts for text columns in grants df\n",
    "grants_text_cols = [\"grant_title\", \"grant_desc\", \"recipient_activities\"]\n",
    "\n",
    "#create columns\n",
    "for col in grants_text_cols:\n",
    "    grants_df[f\"word_count_{col}\"] = grants_df[col].str.split().str.len()\n",
    "for col in grants_text_cols:\n",
    "    print(f\"{col.upper()}\")\n",
    "    print(f\"{'_'*30}\\n\")\n",
    "\n",
    "    word_count_col = f\"word_count_{col}\"\n",
    "    not_nas = grants_df[grants_df[word_count_col].notna()]\n",
    "\n",
    "    #get minimums and maximums for each text column\n",
    "    if len(not_nas) > 0:\n",
    "        min_idx = not_nas[word_count_col].idxmin()\n",
    "        max_idx = not_nas[word_count_col].idxmax()\n",
    "\n",
    "        examples = grants_df.loc[[min_idx, max_idx], [\"funder_num\", \"funder_name\", \"grant_id\", \"recipient_name\", word_count_col, col]]\n",
    "        examples.index = [\"Minimum\", \"Maximum\"]\n",
    "\n",
    "        display(examples)\n",
    "    else:\n",
    "        print(\"No data available\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low word counts - count and view short grant details from 360Giving API/CC public extract\n",
    "grant_details = {\n",
    "    \"grant_title\": \"Short grant title\",\n",
    "    \"grant_desc\": \"Short grant description\",\n",
    "    \"recipient_activities\": \"Short recipient activities\"\n",
    "}\n",
    "\n",
    "for col, title in grant_details.items():\n",
    "    grants_df[f\"short_{col}_section\"] = (\n",
    "        (grants_df[col].str.split().str.len() < 3)\n",
    "    )\n",
    "    short_grant_details = grants_df[grants_df[f\"short_{col}_section\"]][col].dropna().unique()\n",
    "    \n",
    "    # print(f\"\\n{title}: {grants_df[f'short_{col}_section'].sum():,}\")\n",
    "    # print_in_rows(short_grant_details, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high word counts - check top 10 longest grant details (to manually check)\n",
    "long_grant_title = get_longest_values(grants_df, \"word_count_grant_title\", \"grant_id\")\n",
    "long_grant_desc = get_longest_values(grants_df, \"word_count_grant_desc\", \"grant_id\")\n",
    "long_recip_activities = get_longest_values(grants_df, \"word_count_recipient_activities\", \"grant_id\")\n",
    "\n",
    "sections = [\n",
    "    (\"grant_title\", long_grant_title),\n",
    "    (\"grant_desc\", long_grant_desc),\n",
    "    (\"recipient_activities\", long_recip_activities)\n",
    "]\n",
    "\n",
    "for name, longest in sections:\n",
    "    print(f\"Top 10 longest {name}: {longest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Findings from Word Count Analysis (Grants)\n",
    "\n",
    "The word count analysis on the grants dataframe shows that there are almost 2,000 entries with a length of fewer than three words. Having inspected these, I can see that there are a small number of unusual values that need to be cleaned, such as an entry in `recipient_activities` that reads *#NAME?*. There are also several hundred grant titles that appear to be IDs as opposed to descriptive titles (e.g. *VFTF R6-SOLACE-2024*); these will not lend themselves to embedding, so I will remove them. They all appear to end in a dash and a year, so can be identified by \"-2\". \n",
    "\n",
    "In terms of high word counts, there are no concerns. As with the information called from the API in the funders dataframe, the longest grant details are simply relatively long and wordy. I am confident that the information from the 360Giving API is highly reliable, with a very small minority of unexpected values that can be cleaned easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality of Data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data that has been returned from the APIs is generally clean and reliable, there are some minor issues that require tidying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Identified by Word Count Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace ID grant titles with None\n",
    "grants_df.loc[grants_df[\"grant_title\"].str.contains(\"-2\", na=False), \"grant_title\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace unusual values with None\n",
    "unusual_values = [\"#NAME?\", \"NIL ACTIVITY.\", \".\", \"AS BEFORE\", \"IN LIQUIDATION\", \"NONE\", \"0\", \"N/A\"]\n",
    "grants_df[\"grant_desc\"] = grants_df[\"grant_desc\"].replace(unusual_values, None)\n",
    "grants_df[\"recipient_activities\"] = grants_df[\"recipient_activities\"].replace(unusual_values, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print grant details again to confirm cleaning has been successful\n",
    "grant_details = {\n",
    "    \"grant_title\": \"Short grant title\",\n",
    "    \"grant_desc\": \"Short grant description\",\n",
    "    \"recipient_activities\": \"Short recipient activities\"\n",
    "}\n",
    "\n",
    "for col, title in grant_details.items():\n",
    "    grants_df[f\"short_{col}_section\"] = (\n",
    "        (grants_df[col].str.split().str.len() < 3)\n",
    "    )\n",
    "    short_grant_details = grants_df[grants_df[f\"short_{col}_section\"]][col].dropna().unique()\n",
    "    \n",
    "    # print(f\"\\n{title}: {grants_df[f'short_{col}_section'].sum():,}\")\n",
    "    # print_in_rows(short_grant_details, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and Cleaning Other Variables (Funders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check funder names\n",
    "name_lengths = funders_df[\"name\"].str.len()\n",
    "\n",
    "print(f\"Funders with names < 10 characters: {(name_lengths < 10).sum()}\")\n",
    "print(f\"Funders with names > 75 characters: {(name_lengths > 75).sum()}\")\n",
    "\n",
    "print(\"\\nTop 10 shortest funder names (< 10 characters)\")\n",
    "short_names = funders_df[name_lengths < 10][[\"registered_num\", \"name\", \"website\"]].sort_values(\"name\", key=lambda x: x.str.len()).head(10)\n",
    "display(short_names)\n",
    "\n",
    "print(\"\\nTop 10 longest funder names (> 75 characters)\")\n",
    "long_names = funders_df[name_lengths > 100][[\"registered_num\", \"name\", \"website\"]].sort_values(\"name\", key=lambda x: x.str.len(), ascending=False)\n",
    "display(long_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check funder websites\n",
    "website_lengths = funders_df[\"website\"].str.len()\n",
    "\n",
    "print(f\"Funders with websites < 15 characters: {(website_lengths < 15).sum()}\")\n",
    "print(f\"Funders with websites > 75 characters: {(website_lengths > 75).sum()}\")\n",
    "\n",
    "print(\"\\nTop 10 shortest funder websites (< 15 characters)\")\n",
    "short_websites = funders_df[website_lengths < 15][[\"registered_num\", \"name\", \"website\"]].sort_values(\"website\", key=lambda x: x.str.len()).head(10)\n",
    "display(short_websites)\n",
    "\n",
    "print(\"\\nTop 10 longest funder websites (> 75 characters)\")\n",
    "long_websites = funders_df[website_lengths > 75][[\"registered_num\", \"name\", \"website\"]].sort_values(\"website\", key=lambda x: x.str.len(), ascending=False)\n",
    "display(long_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassign null to incorrectly stored urls\n",
    "funders_df.loc[website_lengths < 15, \"website\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and Cleaning Other Variables (Grants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check recipient ids - view ids with unexpected starting characters\n",
    "invalid_ids = grants_df[~grants_df[\"recipient_id\"].str.startswith((\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"P\"), na=False)][\"recipient_id\"].unique()\n",
    "# print(list(invalid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove incorrect spaces and 0s\n",
    "grants_df[\"recipient_id\"] = grants_df[\"recipient_id\"].astype(str).str.strip()\n",
    "grants_df[\"recipient_id\"] = grants_df[\"recipient_id\"].str.lstrip(\"0\")\n",
    "\n",
    "#reassign remaining invalid ids\n",
    "invalid_ids = ~grants_df[\"recipient_id\"].str.startswith((\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"P\"), na=False)\n",
    "grants_df.loc[invalid_ids, \"recipient_id\"] = [f\"invalid_{i}\" for i in range(invalid_ids.sum())]\n",
    "invalid_ids = grants_df[~grants_df[\"recipient_id\"].str.startswith((\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"P\", \"i\"), na=False)][\"recipient_id\"].unique()\n",
    "print(list(invalid_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Cleaning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funder Names and Websites**\n",
    "\n",
    "I am satisfied that there are no concerns with the length of funder names. Looking at the shortest websites, there appeared to be one invalid URL, and five that are stored in an incorrect null format; I reassigned all six of these to None.\n",
    "\n",
    "**Recipient IDs**\n",
    "\n",
    "Some values in `recipient_id` need to be cleaned. There is a small subset with incorrect starting characters, and several that are not registered charity numbers. Working with organisations that are not registered with the Charity Commission (of England & Wales) is out of the scope of this project. Where there is clearly an error in the Charity Commission registered number, I have cleaned these values. I have removed spaces and 0s from the start of IDs and reassigned any other values (e.g. *N/A* or *Exempt*) to show that the ID was invalid. Where `recipient_id` is a company number or a registered number from Scotland or Northern Ireland, I have also assigned an \"invalid\" ID. The text content of these records can be embedded as-is, but anything else (e.g. calling a different API) is out of the scope of the project so no additional value will be gleaned from keeping the original IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality of Extracted Sections and Grants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract sections (i.e `activities_objectives`, `achievements_performance` and `grants_policy`) and grants, I have relied on regex matching and the Claude API (using the Haiku 3 model). Unfortunately, there are noteable errors and quality issues that would degrade the quality of embeddings and undermind the reliability of semantic matching. As such, they cannot be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load reprocessed sections\n",
    "sections_df = pd.read_csv(\"llm_sections.csv\")\n",
    "sections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check efficacy of sections reprocessor\n",
    "sections_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipient Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recipient_name` variable has been built from two sources. The first is the [Charity Commission's Public Extract](https://register-of-charities.charitycommission.gov.uk/en/register/full-register-download), which is updated daily and includes registered charities' names as logged with the Charity Commission. These names are generally clean and standardised, though charities occasionally use trading names that differ from their official registered names.\n",
    "\n",
    "The second source is the organisation's name as extracted from funders' accounts by the Claude API. This is significantly less reliable and is a noted limitation of this project. The reliability is affected in part by the reliance on a large language model and its fuzzy intepretations of messy text, but also due to the fact that funders' accounts are not held to any standard of enforcement in terms of the correct spelling, uniformity, or indeed accuracy of recipients' names. Examples that have been noted during the course of this project include inconsistency in pluralisation and spacing (e.g. *hospices* vs *hospice*, *Tearfund* vs *Tear Fund*); missing words (e.g. *British Red Cross Society* vs *British Red Cross*); errors in punctuation (e.g. *soldiers'* vs *solider's*); and inconsistency in word order (e.g. *Lincoln University* vs *University of Lincoln*).\n",
    "\n",
    "The LLM has also at times failed to interpret account entries properly, extracting purchase descriptions as recipient names (e.g. 'cricket balls for the Year 7 team' from a PTA's accounts) or recording vague summaries as distinct recipients (e.g. 'five various causes'). These errors are particularly prevalent in accounts from single-beneficiary funders where expenditure descriptions differ structurally from multi-recipient grant listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Recipients with weird names\n",
    "- Recipients who are individuals - data protection issue\n",
    "- Unusual characters in text that will be embedded\n",
    "\n",
    "- Single beneficiary funders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Giving Patterns and Funder Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key objectives of prospie is to help fundraisers navigate the confusing trusts landscape, which is confusing largely due to the mismatch of information that is available. Funders may indicate a particular cause or area of activity, but these do not always align with their actual giving habits. I will therefore compare the classifications that funders state (their identified causes, beneficiaries, and areas of activity) with those of the recipients who are awarded their grants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display plots to compare funders' and recipients' classifications\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "#causes\n",
    "make_bar_chart(grants_df, \"recipient_causes\", \"Recipients: Most Popular Causes\", color=\"#2E86AB\", ax=axes[0, 0])\n",
    "make_bar_chart(funders_df, \"causes\", \"Funders: Most Popular Causes\", color=\"#2E86AB\", ax=axes[0, 1])\n",
    "\n",
    "#areas\n",
    "make_bar_chart(grants_df, \"recipient_areas\", \"Recipients: Most Popular Areas\", color=\"#A23B72\", ax=axes[1, 0])\n",
    "make_bar_chart(funders_df, \"areas\", \"Funders: Most Popular Areas\", color=\"#A23B72\", ax=axes[1, 1])\n",
    "\n",
    "#beneficiaries\n",
    "make_bar_chart(grants_df, \"recipient_beneficiaries\", \"Recipients: Most Popular Beneficiaries\", color=\"#F18F01\", ax=axes[2, 0])\n",
    "make_bar_chart(funders_df, \"beneficiaries\", \"Funders: Most Popular Beneficiaries\", color=\"#F18F01\", ax=axes[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that funders often state an interest in general causes, areas and beneficiaries - many do not specify particular interests and state that they will consider applications from any area of the sector. Further analysis will be useful, particularly following the creation of embeddings, to understand the practical reality of their funding priorities, which may reveal implicit preferences or local biases not reflected in their published criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships between Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal and Financial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation with Approach to Embedding and Comparing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "colours = sns.color_palette(\"Set2\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from approach_tester import test_embedding_approach, compare_approaches\n",
    "\n",
    "#get keys from env\n",
    "load_dotenv()\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Data from Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will reuse the checkpoints created in the model selection notebook and use the same data to explore the best approach to comparing the similarity of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get checkpoint folder\n",
    "checkpoint_folder = Path(\"./7.1_checkpoints/\")\n",
    "\n",
    "recipients_df = pd.read_pickle(checkpoint_folder / \"recipients_df.pkl\")\n",
    "funders_df = pd.read_pickle(checkpoint_folder / \"funders_df.pkl\")\n",
    "embedding_pairs = pd.read_pickle(checkpoint_folder / \"embedding_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipients: (12, 6) | Funders: (12, 14) | Evaluation Pairs: (12, 6)\n"
     ]
    }
   ],
   "source": [
    "#check dfs\n",
    "print(f\"Recipients: {recipients_df.shape} | Funders: {funders_df.shape} | Evaluation Pairs: {embedding_pairs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation\n",
    "\n",
    "Based on the model selection notebook, `all-roberta-large-v1` performed best when comparing funder and recipient text. Now I will test different approaches to determine which of the following approaches to embedding/comparison works best:\n",
    "1. Like-for-like comparisons (i.e. funder activities vs recipient activities, or objectives vs objectives)\n",
    "2. Combining some columns (e.g. activities + objectives)\n",
    "3. Combining all available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model constant\n",
    "MODEL = \"all-roberta-large-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Like-for-Like Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will test whether comparing similar sections works best - activities to activities, and objectives to objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities_only: r=0.724, time=2.9s\n",
      "objectives_only: r=0.688, time=4.5s\n",
      "\n",
      "Total time: 7.4s\n",
      "Best approach: activities_only (r=0.724)\n"
     ]
    }
   ],
   "source": [
    "#define like-for-like approaches\n",
    "like_for_like = {\n",
    "    \"activities_only\": ([\"activities\"], [\"recipient_activities\"]),\n",
    "    \"objectives_only\": ([\"objectives\"], [\"recipient_objectives\"])\n",
    "}\n",
    "\n",
    "#test approaches\n",
    "results_lfl, pairs_lfl = compare_approaches(\n",
    "    model_name=MODEL,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    embedding_pairs=embedding_pairs,\n",
    "    approaches_dict=like_for_like\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Combined Column Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will test whether combining activities and objectives - with each other and with other combinations of columns - works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define combined column approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Full Text Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will test whether using all available text columns (including extracted accounts data) provides the most comprehensive comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define full text approaches\n",
    "full_text_approaches = {\n",
    "    \"all_columns\": ([\"activities\", \"objectives\", \"objectives_activities\", \"achievements_performance\", \"grant_policy\"], \n",
    "                    [\"recipient_activities\", \"recipient_objectives\"])\n",
    "}\n",
    "\n",
    "#test approaches\n",
    "results_full, pairs_full = compare_approaches(\n",
    "    model_name=MODEL,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    embedding_pairs=embedding_pairs,\n",
    "    approaches_dict=full_text_approaches\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation with Approach to Embedding and Comparing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "colours = sns.color_palette(\"Set2\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from approach_tester import compare_approaches, COMBINATIONS_DICT, RECIPIENTS_SECTIONS\n",
    "\n",
    "#get keys from env\n",
    "load_dotenv()\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Data from Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will reuse the checkpoints created in the model selection notebook and use the same data to explore the best approach to comparing the similarity of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get checkpoint folder\n",
    "checkpoint_folder = Path(\"./7.1_checkpoints/\")\n",
    "\n",
    "recipients_df = pd.read_pickle(checkpoint_folder / \"recipients_df.pkl\")\n",
    "funders_df = pd.read_pickle(checkpoint_folder / \"funders_df.pkl\")\n",
    "embedding_pairs = pd.read_pickle(checkpoint_folder / \"embedding_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipients: (12, 6) | Funders: (12, 14) | Evaluation Pairs: (12, 6)\n"
     ]
    }
   ],
   "source": [
    "#check dfs\n",
    "print(f\"Recipients: {recipients_df.shape} | Funders: {funders_df.shape} | Evaluation Pairs: {embedding_pairs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation - Recipient Text as Independent Variable\n",
    "\n",
    "Based on the model selection notebook, `all-roberta-large-v1` performed best when comparing funder and recipient text. Now I will test different approaches to determine which of the following approaches to embedding/comparison works best:\n",
    "1. Like-for-like comparisons (i.e. funder activities vs recipient activities, or objectives vs objectives)\n",
    "2. Combining some columns (e.g. activities + objectives)\n",
    "3. Combining all available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model constant\n",
    "MODEL = \"all-roberta-large-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Like-for-Like Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities_only: r=0.724, time=4.5s\n",
      "objectives_only: r=0.688, time=5.2s\n",
      "\n",
      "Total time: 9.7s\n",
      "Best approach(es):\n",
      "  activities_only: r=0.724, time=4.5s\n",
      "  objectives_only: r=0.688, time=5.2s\n"
     ]
    }
   ],
   "source": [
    "#define like-for-like approaches\n",
    "like_for_like = {\n",
    "    \"activities_only\": ([\"activities\"], [\"recipient_activities\"]),\n",
    "    \"objectives_only\": ([\"objectives\"], [\"recipient_objectives\"])\n",
    "}\n",
    "\n",
    "#test approaches\n",
    "results_lfl, pairs_lfl = compare_approaches(\n",
    "    model_name=MODEL,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    embedding_pairs=embedding_pairs,\n",
    "    approaches_dict=like_for_like\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Combined Column Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acts_objs: r=0.688, time=5.7s\n",
      "acts_objacts: r=0.855, time=5.2s\n",
      "acts_achs: r=0.860, time=6.2s\n",
      "acts_policy: r=0.919, time=9.3s\n",
      "objs_objacts: r=0.812, time=6.2s\n",
      "objs_achs: r=0.823, time=6.0s\n",
      "objs_policy: r=0.905, time=7.1s\n",
      "acts_objs_objacts: r=0.803, time=5.9s\n",
      "acts_objs_achs: r=0.865, time=44.2s\n",
      "acts_objs_policy: r=0.871, time=59.1s\n",
      "acts_objacts_achs: r=0.824, time=17.4s\n",
      "acts_objacts_policy: r=0.859, time=13.4s\n",
      "acts_achs_policy: r=0.873, time=14.3s\n",
      "objs_objacts_achs: r=0.811, time=22.6s\n",
      "objs_objacts_policy: r=0.858, time=16.3s\n",
      "objs_achs_policy: r=0.813, time=47.3s\n",
      "acts_objs_objacts_achs: r=0.828, time=36.6s\n",
      "acts_objs_objacts_policy: r=0.844, time=31.2s\n",
      "acts_objs_achs_policy: r=0.872, time=10.2s\n",
      "acts_objacts_achs_policy: r=0.830, time=7.6s\n",
      "objs_objacts_achs_policy: r=0.802, time=18.5s\n",
      "\n",
      "Total time: 390.5s\n",
      "Best approach(es):\n",
      "  acts_policy: r=0.919, time=9.3s\n",
      "  objs_policy: r=0.905, time=7.1s\n",
      "  acts_achs_policy: r=0.873, time=14.3s\n",
      "  acts_objs_achs_policy: r=0.872, time=10.2s\n",
      "  acts_objs_policy: r=0.871, time=59.1s\n"
     ]
    }
   ],
   "source": [
    "#test combined column approaches\n",
    "results_combined, pairs_combined = compare_approaches(\n",
    "    model_name=MODEL,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    embedding_pairs=embedding_pairs,\n",
    "    approaches_dict=COMBINATIONS_DICT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Full Text Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_columns: r=0.831, time=6.4s\n",
      "\n",
      "Total time: 6.4s\n",
      "Best approach(es):\n",
      "  all_columns: r=0.831, time=6.4s\n"
     ]
    }
   ],
   "source": [
    "#define full text approaches\n",
    "full_text = {\n",
    "    \"all_columns\": ([\"activities\", \"objectives\", \"objectives_activities\", \"achievements_performance\", \"grant_policy\"], RECIPIENTS_SECTIONS)\n",
    "}\n",
    "\n",
    "#test approaches\n",
    "results_full, pairs_full = compare_approaches(\n",
    "    model_name=MODEL,\n",
    "    funders_df=funders_df,\n",
    "    recipients_df=recipients_df,\n",
    "    embedding_pairs=embedding_pairs,\n",
    "    approaches_dict=full_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Whilst the combination of `activities` and `grant_policy` achieved the highest correlation (r=0.92), I would hesitate to use this for the final model due to concerns about missing context. From the top 5 results, I will select `acts_objs_achs_policy` (r=0.87). This decision reflects:\n",
    "\n",
    "- The small sample size of these experiments (n=12) means that the 5-point difference between the first and fourth best performers may not be reliable.\n",
    "- That said, a score of 0.87 still represents a very strong correlation between the embeddings and my ratings.\n",
    "- Domain expertise: in UK trust fundraising, activities data is critical for alignment assessment - funders explicitly evaluate what organisations do, not just what they aim to achieve. I am confident that excluding both activities and objectives would be inadvisable due to the risk of losing important contextual data.\n",
    "- The four-way combination of `activities`, `objectives`, `achievements_performance` and `grant_policy` achieved essentially identical performance to the third-best combination (r=0.872 vs r=0.873 respectively), whilst reducing computation time by approximately 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation - Funder Text as Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will test the best performing approaches against different combinations of the recipient sections, to evaluate whether separating our `recipient_activities` and `recipient_objectives` produces better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5 - Recipient Activities Text Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6 - Recipient Objectives Text Only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
